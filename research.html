<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/e81551aa64f804d1.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-b62247d9222ab883.js"/><script src="/_next/static/chunks/4bd1b696-ea8457096315284e.js" async=""></script><script src="/_next/static/chunks/684-786dac65ce06c12d.js" async=""></script><script src="/_next/static/chunks/main-app-89a747995656dcfe.js" async=""></script><script src="/_next/static/chunks/874-58d2f7b217ecad8c.js" async=""></script><script src="/_next/static/chunks/app/layout-206c9fd67d3c2f16.js" async=""></script><script src="/_next/static/chunks/63-da59d4f3b5da0c11.js" async=""></script><script src="/_next/static/chunks/app/research/page-e612ed3eeae59717.js" async=""></script><title>Research | nayemislamzr</title><meta name="description" content="Nayem Islam ZR&#x27;s personal portfolio website."/><link rel="icon" href="/favicon.ico" type="image/x-icon" sizes="16x16"/><link rel="icon" href="/images/favicon_io/favicon.ico" sizes="any"/><link rel="icon" href="/images/favicon_io/favicon-16x16.png" type="image/png" sizes="16x16"/><link rel="icon" href="/images/favicon_io/favicon-32x32.png" type="image/png" sizes="32x32"/><link rel="apple-touch-icon" href="/images/favicon_io/apple-touch-icon.png"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="bg-gray-100"> <!-- --> <nav class="sticky top-0 z-[999] flex items-center w-full px-4 md:px-8 h-[70px] bg-white border-b-2 border-black shadow-md"><header class="flex w-full items-center justify-between"><div class="flex items-center"></div><nav aria-label="Main" class="flex-1 items-center justify-center hidden lg:flex"><ul class="group flex list-none items-center justify-center space-x-1"><li><a class="group inline-flex h-9 w-max items-center justify-center px-4 py-2 text-sm font-bold transition-colors focus:outline-none border-2 text-black border-transparent hover:bg-orange-300 hover:border-black" href="/">Home</a></li><li><a class="group inline-flex h-9 w-max items-center justify-center px-4 py-2 text-sm font-bold transition-colors focus:outline-none border-2 text-black border-transparent hover:bg-orange-300 hover:border-black" href="/resume">Resume</a></li><li><a class="group inline-flex h-9 w-max items-center justify-center px-4 py-2 text-sm font-bold transition-colors focus:outline-none border-2 text-black border-transparent hover:bg-orange-300 hover:border-black" href="/projects">Projects</a></li><li><a class="group inline-flex h-9 w-max items-center justify-center px-4 py-2 text-sm font-bold transition-colors focus:outline-none border-2 text-black border-transparent hover:bg-orange-300 hover:border-black" href="/achievements">Achievements</a></li><li><a class="group inline-flex h-9 w-max items-center justify-center px-4 py-2 text-sm font-bold transition-colors focus:outline-none border-2 text-black border-transparent hover:bg-orange-300 hover:border-black" href="/academic">Academics</a></li><li><a class="group inline-flex h-9 w-max items-center justify-center px-4 py-2 text-sm font-bold transition-colors focus:outline-none border-2 bg-orange-300 text-black border-black" href="/research">Research</a></li><li><a class="group inline-flex h-9 w-max items-center justify-center px-4 py-2 text-sm font-bold transition-colors focus:outline-none border-2 text-black border-transparent hover:bg-orange-300 hover:border-black" href="/coding">Coding</a></li></ul></nav><div class="flex items-center gap-4"><button class="lg:hidden flex flex-col space-y-1.5 p-2 border-2 border-black bg-orange-300" aria-label="Toggle Menu"><div class="w-6 h-0.5 bg-black"></div><div class="w-6 h-0.5 bg-black"></div><div class="w-6 h-0.5 bg-black"></div></button></div></header></nav><div class="container mx-auto px-2 py-6 sm:px-6 lg:px-8"> <main class="w-full max-w-4xl mx-auto"> <div class="container mx-auto bg-gray-100"> <div class="flex flex-col items-center gap-8 w-full"><div class="flex flex-col flex-grow bg-white border-2 border-black rounded-none shadow-md w-full max-w-3xl overflow-hidden"> <div class="relative w-full aspect-video border-b-2 border-black"><img alt="BongoFashion: A Versatile Dataset for Real-time Detection and Classification of Bangladeshi Wearables  (2024) visual representation" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/images/iccit/conference.jpg"/></div><div class="flex flex-col gap-4 p-4 md:p-6"><h1 class="text-base md:text-xl font-bold text-black text-left">BongoFashion: A Versatile Dataset for Real-time Detection and Classification of Bangladeshi Wearables  (2024)</h1><div class="text-sm md:text-base text-black text-left font-sans"><span class="font-semibold">Authors:</span> <!-- -->MD. Nayem Islam, Ariful Islam Farhad</div><div class="text-sm md:text-base text-black text-left font-sans"><span class="font-semibold">Published in:</span> <!-- -->27th international conference on computer and information technology (ICCIT)</div><div class="text-sm md:text-base text-black text-left font-sans"><span class="font-semibold">Supervisor:</span> <!-- -->Mohammad Shahidur Rahman, PhD</div><p class="text-sm md:text-base text-black text-left font-sans leading-relaxed">Abstract—In this paper, we present BongoFashion, the largest
 dataset of Bangladeshi clothing and fashion accessories, contain
ing 11,498 images across 39 categories of commonly worn items.
 Our dataset captures the rich diversity of Bangladeshi wearables,
 including both traditional and modern attire, in real-world
 settings with varied backgrounds, resolutions, and poses. We
 trained and compared multiple object detection models, including
 six YOLO variants (YOLOv5 to YOLOv10) and the Detectron2
 framework, in two experimental settings: one using the original
 dataset and another applying data augmentations to our training
 dataset. Our experiments revealed that augmentations improve
 model performance, with the YOLOv9 model achieving the
 highest mean average precision (mAP) of 91.7%. This dataset
 and our findings offer valuable resources for advancing wearable
 detection and classification, with promising applications in e
commerce, fashion search engines, and automated clothing cate
gorization systems.</p><div class="text-xs md:text-sm font-mono text-black flex flex-wrap gap-2 items-center"><code class="bg-white border border-black p-1 px-2">Bangladeshi Wearables</code><code class="bg-white border border-black p-1 px-2">Fashion Accessories</code><code class="bg-white border border-black p-1 px-2">Object Detection</code><code class="bg-white border border-black p-1 px-2">Clothing Detection</code><code class="bg-white border border-black p-1 px-2">Clothing Classification</code><code class="bg-white border border-black p-1 px-2">YOLOv9</code><code class="bg-white border border-black p-1 px-2">YOLOv10</code><code class="bg-white border border-black p-1 px-2">Detectron2</code><code class="bg-white border border-black p-1 px-2">Deep Learning</code><code class="bg-white border border-black p-1 px-2">Image Recognition</code></div><div class="flex justify-center items-center border-t-2 border-black pt-4 mt-auto"> <div class="flex flex-row gap-4 items-center justify-center border-2 border-black rounded-full px-3 py-1"><a target="_blank" rel="noopener noreferrer" class="text-black hover:opacity-75 transition-opacity" href="https://drive.google.com/file/d/1OAtw3k61vILLWuaUcjO-GDas5de9sE8w/view"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link h-5 w-5" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" color="black"><path fill="currentColor" d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c28.5 28.5 28.5 74.8 0 103.3L325.1 369.8c-28.5 28.5-74.8 28.5-103.3 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C60.2 227.2 53.7 306 103.7 356.1c56.5 56.5 148 56.5 204.5 0L459.7 267.7z"></path></svg> </a> </div></div> </div></div><div class="flex flex-col flex-grow bg-white border-2 border-black rounded-none shadow-md w-full max-w-3xl overflow-hidden"> <div class="relative w-full aspect-video border-b-2 border-black"><img alt="Math101: A Novel Approach to Optimizing LLMs Using Block-D and Pruning visual representation" loading="lazy" decoding="async" data-nimg="fill" class="object-cover" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/images/dl-sprint-3/team_photo2.jpeg"/></div><div class="flex flex-col gap-4 p-4 md:p-6"><h1 class="text-base md:text-xl font-bold text-black text-left">Math101: A Novel Approach to Optimizing LLMs Using Block-D and Pruning</h1><div class="text-sm md:text-base text-black text-left font-sans"><span class="font-semibold">Authors:</span> <!-- -->Kawchar Husain, MD. Nayem Islam, Ariful Islam Farhad</div><div class="text-sm md:text-base text-black text-left font-sans"><span class="font-semibold">Published in:</span> <!-- -->DL Sprint 3.0 — Bengali AI Math Olympiad (2024)</div><p class="text-sm md:text-base text-black text-left font-sans leading-relaxed">Abstract—Recent improvements in large language models
(LLMs) have significantly advanced their effectiveness in handling complex mathematical reasoning tasks. However, achieving efficient inference and managing computational costs remain major challenges, especially in resource-limited environments.

This work presents an optimized framework that enhances inference speed through a dual-pruning strategy—Dominance-Based Pruning and Threshold-Based Pruning—combined with a Block Decomposition (Block-D) technique for sequential sample pro-
cessing. By organizing problem-solving into manageable blocks, our method minimizes redundant computations, particularly for
simpler problems. Our approach demonstrates a 4x increase in
inference speed. In testing, 73% of problems were pruned using
only the first block, maintaining an accuracy rate of 92% for
these cases, and achieving an overall model accuracy of 84%.</p><div class="text-xs md:text-sm font-mono text-black flex flex-wrap gap-2 items-center"><code class="bg-white border border-black p-1 px-2">Inference Optimization</code><code class="bg-white border border-black p-1 px-2">Mathematical Reasoning</code><code class="bg-white border border-black p-1 px-2">Bengali Math Problem-Solving</code><code class="bg-white border border-black p-1 px-2">Self-Consistency</code><code class="bg-white border border-black p-1 px-2">Tool-Integrated Reasoning (TIR)</code><code class="bg-white border border-black p-1 px-2">Chain of Thought (CoT)</code><code class="bg-white border border-black p-1 px-2">Program of Thought (PoT)</code></div><div class="flex justify-center items-center border-t-2 border-black pt-4 mt-auto"> <div class="flex flex-row gap-4 items-center justify-center border-2 border-black rounded-full px-3 py-1"><a target="_blank" rel="noopener noreferrer" class="text-black hover:opacity-75 transition-opacity" href="https://drive.google.com/file/d/104aOs0PB3hamEdFtRfT_35-wGiu-rtdC/view"><svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="link" class="svg-inline--fa fa-link h-5 w-5" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512" color="black"><path fill="currentColor" d="M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c28.5 28.5 28.5 74.8 0 103.3L325.1 369.8c-28.5 28.5-74.8 28.5-103.3 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C60.2 227.2 53.7 306 103.7 356.1c56.5 56.5 148 56.5 204.5 0L459.7 267.7z"></path></svg> </a> </div></div> </div></div></div></div></main></div><script src="/_next/static/chunks/webpack-b62247d9222ab883.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[5494,[\"874\",\"static/chunks/874-58d2f7b217ecad8c.js\",\"177\",\"static/chunks/app/layout-206c9fd67d3c2f16.js\"],\"default\"]\n3:I[8691,[\"874\",\"static/chunks/874-58d2f7b217ecad8c.js\",\"177\",\"static/chunks/app/layout-206c9fd67d3c2f16.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[3063,[\"63\",\"static/chunks/63-da59d4f3b5da0c11.js\",\"874\",\"static/chunks/874-58d2f7b217ecad8c.js\",\"322\",\"static/chunks/app/research/page-e612ed3eeae59717.js\"],\"Image\"]\n8:I[6874,[\"63\",\"static/chunks/63-da59d4f3b5da0c11.js\",\"874\",\"static/chunks/874-58d2f7b217ecad8c.js\",\"322\",\"static/chunks/app/research/page-e612ed3eeae59717.js\"],\"\"]\n9:I[9665,[],\"OutletBoundary\"]\nc:I[9665,[],\"ViewportBoundary\"]\ne:I[9665,[],\"MetadataBoundary\"]\n10:I[6614,[],\"\"]\n:HL[\"/_next/static/css/e81551aa64f804d1.css\",\"style\"]\n7:T402,Abstract—In this paper, we present BongoFashion, the largest\n dataset of Bangladeshi clothing and fashion accessories, contain\ning 11,498 images across 39 categories of commonly worn items.\n Our dataset captures the rich diversity of Bangladeshi wearables,\n including both traditional and modern attire, in real-world\n settings with varied backgrounds, resolutions, and poses. We\n trained and compared multiple object detection models, including\n six YOLO variants (YOLOv5 to YOLOv10) and the Detectron2\n framework, in two experimental settings: one using the original\n dataset and another applying data augmentations to our training\n dataset. Our experiments revealed that augmentations improve\n model performance, with the YOLOv9 model achieving the\n highest mean average precision (mAP) of 91.7%. This dataset\n and our findings offer valuable resources for advancing wearable\n detection and classification, with promising applications in e\ncommerce, fashion search engines, and automated clothing cate\ngorization systems."])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"K_B3X3MlXl9GfDCEkUWdU\",\"p\":\"\",\"c\":[\"\",\"research\"],\"i\":false,\"f\":[[[\"\",{\"children\":[\"research\",{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/e81551aa64f804d1.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"children\":[\"$\",\"body\",null,{\"className\":\"bg-gray-100\",\"children\":[\" \",[\"$\",\"$L2\",null,{}],[\"$\",\"$L3\",null,{\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}]]}]}]]}],{\"children\":[\"research\",[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"container mx-auto bg-gray-100\",\"children\":[\" \",[\"$\",\"div\",null,{\"className\":\"flex flex-col items-center gap-8 w-full\",\"children\":[[\"$\",\"div\",\"1\",{\"className\":\"flex flex-col flex-grow bg-white border-2 border-black rounded-none shadow-md w-full max-w-3xl overflow-hidden\",\"children\":[\" \",[\"$\",\"div\",null,{\"className\":\"relative w-full aspect-video border-b-2 border-black\",\"children\":[\"$\",\"$L6\",null,{\"src\":\"/images/iccit/conference.jpg\",\"alt\":\"BongoFashion: A Versatile Dataset for Real-time Detection and Classification of Bangladeshi Wearables  (2024) visual representation\",\"fill\":true,\"className\":\"object-cover\",\"loading\":\"lazy\"}]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 p-4 md:p-6\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-base md:text-xl font-bold text-black text-left\",\"children\":\"BongoFashion: A Versatile Dataset for Real-time Detection and Classification of Bangladeshi Wearables  (2024)\"}],[\"$\",\"div\",null,{\"className\":\"text-sm md:text-base text-black text-left font-sans\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Authors:\"}],\" \",\"MD. Nayem Islam, Ariful Islam Farhad\"]}],[\"$\",\"div\",null,{\"className\":\"text-sm md:text-base text-black text-left font-sans\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Published in:\"}],\" \",\"27th international conference on computer and information technology (ICCIT)\"]}],[\"$\",\"div\",null,{\"className\":\"text-sm md:text-base text-black text-left font-sans\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Supervisor:\"}],\" \",\"Mohammad Shahidur Rahman, PhD\"]}],[\"$\",\"p\",null,{\"className\":\"text-sm md:text-base text-black text-left font-sans leading-relaxed\",\"children\":\"$7\"}],[\"$\",\"div\",null,{\"className\":\"text-xs md:text-sm font-mono text-black flex flex-wrap gap-2 items-center\",\"children\":[[\"$\",\"code\",\"0\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Bangladeshi Wearables\"}],[\"$\",\"code\",\"1\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Fashion Accessories\"}],[\"$\",\"code\",\"2\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Object Detection\"}],[\"$\",\"code\",\"3\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Clothing Detection\"}],[\"$\",\"code\",\"4\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Clothing Classification\"}],[\"$\",\"code\",\"5\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"YOLOv9\"}],[\"$\",\"code\",\"6\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"YOLOv10\"}],[\"$\",\"code\",\"7\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Detectron2\"}],[\"$\",\"code\",\"8\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Deep Learning\"}],[\"$\",\"code\",\"9\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Image Recognition\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex justify-center items-center border-t-2 border-black pt-4 mt-auto\",\"children\":[\" \",[\"$\",\"div\",null,{\"className\":\"flex flex-row gap-4 items-center justify-center border-2 border-black rounded-full px-3 py-1\",\"children\":[\"$undefined\",[\"$\",\"$L8\",\"paper-link\",{\"href\":\"https://drive.google.com/file/d/1OAtw3k61vILLWuaUcjO-GDas5de9sE8w/view\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-black hover:opacity-75 transition-opacity\",\"children\":[[\"$\",\"svg\",null,{\"aria-hidden\":\"true\",\"focusable\":\"false\",\"data-prefix\":\"fas\",\"data-icon\":\"link\",\"className\":\"svg-inline--fa fa-link h-5 w-5\",\"role\":\"img\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 640 512\",\"color\":\"black\",\"children\":[\"$\",\"path\",null,{\"fill\":\"currentColor\",\"d\":\"M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c28.5 28.5 28.5 74.8 0 103.3L325.1 369.8c-28.5 28.5-74.8 28.5-103.3 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C60.2 227.2 53.7 306 103.7 356.1c56.5 56.5 148 56.5 204.5 0L459.7 267.7z\"}]}],\" \"]}],\" \"]}]]}],\" \"]}]]}],[\"$\",\"div\",\"2\",{\"className\":\"flex flex-col flex-grow bg-white border-2 border-black rounded-none shadow-md w-full max-w-3xl overflow-hidden\",\"children\":[\" \",[\"$\",\"div\",null,{\"className\":\"relative w-full aspect-video border-b-2 border-black\",\"children\":[\"$\",\"$L6\",null,{\"src\":\"/images/dl-sprint-3/team_photo2.jpeg\",\"alt\":\"Math101: A Novel Approach to Optimizing LLMs Using Block-D and Pruning visual representation\",\"fill\":true,\"className\":\"object-cover\",\"loading\":\"lazy\"}]}],[\"$\",\"div\",null,{\"className\":\"flex flex-col gap-4 p-4 md:p-6\",\"children\":[[\"$\",\"h1\",null,{\"className\":\"text-base md:text-xl font-bold text-black text-left\",\"children\":\"Math101: A Novel Approach to Optimizing LLMs Using Block-D and Pruning\"}],[\"$\",\"div\",null,{\"className\":\"text-sm md:text-base text-black text-left font-sans\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Authors:\"}],\" \",\"Kawchar Husain, MD. Nayem Islam, Ariful Islam Farhad\"]}],[\"$\",\"div\",null,{\"className\":\"text-sm md:text-base text-black text-left font-sans\",\"children\":[[\"$\",\"span\",null,{\"className\":\"font-semibold\",\"children\":\"Published in:\"}],\" \",\"DL Sprint 3.0 — Bengali AI Math Olympiad (2024)\"]}],\"$undefined\",[\"$\",\"p\",null,{\"className\":\"text-sm md:text-base text-black text-left font-sans leading-relaxed\",\"children\":\"Abstract—Recent improvements in large language models\\n(LLMs) have significantly advanced their effectiveness in handling complex mathematical reasoning tasks. However, achieving efficient inference and managing computational costs remain major challenges, especially in resource-limited environments.\\n\\nThis work presents an optimized framework that enhances inference speed through a dual-pruning strategy—Dominance-Based Pruning and Threshold-Based Pruning—combined with a Block Decomposition (Block-D) technique for sequential sample pro-\\ncessing. By organizing problem-solving into manageable blocks, our method minimizes redundant computations, particularly for\\nsimpler problems. Our approach demonstrates a 4x increase in\\ninference speed. In testing, 73% of problems were pruned using\\nonly the first block, maintaining an accuracy rate of 92% for\\nthese cases, and achieving an overall model accuracy of 84%.\"}],[\"$\",\"div\",null,{\"className\":\"text-xs md:text-sm font-mono text-black flex flex-wrap gap-2 items-center\",\"children\":[[\"$\",\"code\",\"0\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Inference Optimization\"}],[\"$\",\"code\",\"1\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Mathematical Reasoning\"}],[\"$\",\"code\",\"2\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Bengali Math Problem-Solving\"}],[\"$\",\"code\",\"3\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Self-Consistency\"}],[\"$\",\"code\",\"4\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Tool-Integrated Reasoning (TIR)\"}],[\"$\",\"code\",\"5\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Chain of Thought (CoT)\"}],[\"$\",\"code\",\"6\",{\"className\":\"bg-white border border-black p-1 px-2\",\"children\":\"Program of Thought (PoT)\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex justify-center items-center border-t-2 border-black pt-4 mt-auto\",\"children\":[\" \",[\"$\",\"div\",null,{\"className\":\"flex flex-row gap-4 items-center justify-center border-2 border-black rounded-full px-3 py-1\",\"children\":[\"$undefined\",[\"$\",\"$L8\",\"paper-link\",{\"href\":\"https://drive.google.com/file/d/104aOs0PB3hamEdFtRfT_35-wGiu-rtdC/view\",\"target\":\"_blank\",\"rel\":\"noopener noreferrer\",\"className\":\"text-black hover:opacity-75 transition-opacity\",\"children\":[[\"$\",\"svg\",null,{\"aria-hidden\":\"true\",\"focusable\":\"false\",\"data-prefix\":\"fas\",\"data-icon\":\"link\",\"className\":\"svg-inline--fa fa-link h-5 w-5\",\"role\":\"img\",\"xmlns\":\"http://www.w3.org/2000/svg\",\"viewBox\":\"0 0 640 512\",\"color\":\"black\",\"children\":[\"$\",\"path\",null,{\"fill\":\"currentColor\",\"d\":\"M579.8 267.7c56.5-56.5 56.5-148 0-204.5c-50-50-128.8-56.5-186.3-15.4l-1.6 1.1c-14.4 10.3-17.7 30.3-7.4 44.6s30.3 17.7 44.6 7.4l1.6-1.1c32.1-22.9 76-19.3 103.8 8.6c28.5 28.5 28.5 74.8 0 103.3L325.1 369.8c-28.5 28.5-74.8 28.5-103.3 0c-27.9-27.9-31.5-71.8-8.6-103.8l1.1-1.6c10.3-14.4 6.9-34.4-7.4-44.6s-34.4-6.9-44.6 7.4l-1.1 1.6C60.2 227.2 53.7 306 103.7 356.1c56.5 56.5 148 56.5 204.5 0L459.7 267.7z\"}]}],\" \"]}],\" \"]}]]}],\" \"]}]]}]]}],false]}],\"$undefined\",null,[\"$\",\"$L9\",null,{\"children\":[\"$La\",\"$Lb\",null]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"Nmqz4hmnfCst8Vi7vtNAD\",{\"children\":[[\"$\",\"$Lc\",null,{\"children\":\"$Ld\"}],null]}],[\"$\",\"$Le\",null,{\"children\":\"$Lf\"}]]}],false]],\"m\":\"$undefined\",\"G\":[\"$10\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\na:null\n"])</script><script>self.__next_f.push([1,"b:null\nf:[[\"$\",\"title\",\"0\",{\"children\":\"Research | nayemislamzr\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Nayem Islam ZR's personal portfolio website.\"}],[\"$\",\"link\",\"2\",{\"rel\":\"icon\",\"href\":\"/favicon.ico\",\"type\":\"image/x-icon\",\"sizes\":\"16x16\"}],[\"$\",\"link\",\"3\",{\"rel\":\"icon\",\"href\":\"/images/favicon_io/favicon.ico\",\"sizes\":\"any\"}],[\"$\",\"link\",\"4\",{\"rel\":\"icon\",\"href\":\"/images/favicon_io/favicon-16x16.png\",\"type\":\"image/png\",\"sizes\":\"16x16\"}],[\"$\",\"link\",\"5\",{\"rel\":\"icon\",\"href\":\"/images/favicon_io/favicon-32x32.png\",\"type\":\"image/png\",\"sizes\":\"32x32\"}],[\"$\",\"link\",\"6\",{\"rel\":\"apple-touch-icon\",\"href\":\"/images/favicon_io/apple-touch-icon.png\"}]]\n"])</script></body></html>